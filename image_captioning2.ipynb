{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458938f7-b0d7-4b8b-be27-d893472d36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install modin[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26488802-0cea-4184-bbfd-691ef29f3494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: tensorflow 2.14.1\n",
      "Uninstalling tensorflow-2.14.1:\n",
      "  Successfully uninstalled tensorflow-2.14.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping estimator as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: keras 2.14.0\n",
      "Uninstalling keras-2.14.0:\n",
      "  Successfully uninstalled keras-2.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow estimator keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db312d23-beeb-4ec9-bfc9-9ac4b90bb70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow_text in ./.local/lib/python3.9/site-packages (2.15.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "Requirement already satisfied: tensorflow_datasets in ./.local/lib/python3.9/site-packages (4.9.3)\n",
      "Requirement already satisfied: tensorflow-hub>=0.13.0 in ./.local/lib/python3.9/site-packages (from tensorflow_text) (0.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.local/lib/python3.9/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.local/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.local/lib/python3.9/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.local/lib/python3.9/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.local/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./.local/lib/python3.9/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.local/lib/python3.9/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in ./.local/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.local/lib/python3.9/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.9/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.local/lib/python3.9/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./.local/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.local/lib/python3.9/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.local/lib/python3.9/site-packages (from tensorflow) (1.60.1)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: array-record in ./.local/lib/python3.9/site-packages (from tensorflow_datasets) (0.5.0)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.9/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in ./.local/lib/python3.9/site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in ./.local/lib/python3.9/site-packages (from tensorflow_datasets) (1.5.2)\n",
      "Requirement already satisfied: promise in ./.local/lib/python3.9/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-metadata in ./.local/lib/python3.9/site-packages (from tensorflow_datasets) (1.14.0)\n",
      "Requirement already satisfied: toml in ./.local/lib/python3.9/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (from tensorflow_datasets) (4.66.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2024.2.0)\n",
      "Requirement already satisfied: importlib_resources in ./.local/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.1.1)\n",
      "Requirement already satisfied: zipp in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.7.22)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in ./.local/lib/python3.9/site-packages (from tensorflow-hub>=0.13.0->tensorflow_text) (2.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in ./.local/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.62.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorflow-estimator, keras, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.14.0\n",
      "    Uninstalling tensorflow-estimator-2.14.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.14.1\n",
      "    Uninstalling tensorboard-2.14.1:\n",
      "      Successfully uninstalled tensorboard-2.14.1\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "intel-extension-for-tensorflow 2.14.0.2 requires tensorflow~=2.14.0, but you have tensorflow 2.15.0.post1 which is incompatible.\n",
      "intel-tensorflow 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.15.0 which is incompatible.\n",
      "intel-tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.15.1 which is incompatible.\n",
      "intel-tensorflow 2.14.0 requires tensorflow-estimator<2.15,>=2.14.0, but you have tensorflow-estimator 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "312515c5-b751-46c5-be08-5f07385d360a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: intel-tensorflow in ./.local/lib/python3.9/site-packages (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel-tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel-tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel-tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel-tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.local/lib/python3.9/site-packages (from intel-tensorflow) (1.60.1)\n",
      "Collecting tensorboard<2.15,>=2.14 (from intel-tensorflow)\n",
      "  Using cached tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from intel-tensorflow)\n",
      "  Using cached tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from intel-tensorflow)\n",
      "  Using cached keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->intel-tensorflow) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->intel-tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->intel-tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->intel-tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->intel-tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->intel-tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->intel-tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->intel-tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->intel-tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->intel-tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->intel-tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->intel-tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->intel-tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->intel-tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->intel-tensorflow) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->intel-tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->intel-tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->intel-tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->intel-tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->intel-tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorflow-estimator, keras, tensorboard\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.15.0\n",
      "    Uninstalling tensorflow-estimator-2.15.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.1\n",
      "    Uninstalling tensorboard-2.15.1:\n",
      "      Successfully uninstalled tensorboard-2.15.1\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0.post1 requires keras<2.16,>=2.15.0, but you have keras 2.14.0 which is incompatible.\n",
      "tensorflow 2.15.0.post1 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.14.1 which is incompatible.\n",
      "tensorflow 2.15.0.post1 requires tensorflow-estimator<2.16,>=2.15.0, but you have tensorflow-estimator 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.14.0 tensorboard-2.14.1 tensorflow-estimator-2.14.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install intel-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9c0011-849f-4d36-94c1-d5ebbbe0fafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: intel-extension-for-tensorflow[cpu] in ./.local/lib/python3.9/site-packages (2.14.0.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./.local/lib/python3.9/site-packages (from intel-extension-for-tensorflow[cpu]) (1.60.1)\n",
      "Requirement already satisfied: wheel in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel-extension-for-tensorflow[cpu]) (0.40.0)\n",
      "Collecting tensorflow~=2.14.0 (from intel-extension-for-tensorflow[cpu])\n",
      "  Using cached tensorflow-2.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
      "Requirement already satisfied: numpy<1.25 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel-extension-for-tensorflow[cpu]) (1.24.3)\n",
      "Requirement already satisfied: protobuf<4.24 in ./.local/lib/python3.9/site-packages (from intel-extension-for-tensorflow[cpu]) (3.20.3)\n",
      "Requirement already satisfied: absl-py==1.4.0 in ./.local/lib/python3.9/site-packages (from intel-extension-for-tensorflow[cpu]) (1.4.0)\n",
      "Requirement already satisfied: intel-extension-for-tensorflow-lib==2.14.0.2.0 in ./.local/lib/python3.9/site-packages (from intel-extension-for-tensorflow[cpu]) (2.14.0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (23.1)\n",
      "Requirement already satisfied: setuptools in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (0.36.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in ./.local/lib/python3.9/site-packages (from tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2.14.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.local/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->intel-extension-for-tensorflow[cpu]) (3.2.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorflow\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0.post1\n",
      "    Uninstalling tensorflow-2.15.0.post1:\n",
      "      Successfully uninstalled tensorflow-2.15.0.post1\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tensorflow-2.14.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade intel-extension-for-tensorflow[cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5bd96d6-9edc-480b-8274-576772f1d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 03:33:38.814636: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-08 03:33:38.931192: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-08 03:33:39.463742: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-08 03:33:39.463762: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-08 03:33:39.467556: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-08 03:33:39.759076: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-08 03:33:39.759999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-08 03:33:41.927228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-02-08 03:33:44.139403: I itex/core/wrapper/itex_cpu_wrapper.cc:60] Intel Extension for Tensorflow* AVX512 CPU backend is loaded.\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "import modin.pandas as pd\n",
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9821a08c-d1a9-49e2-aa97-a1fc6aa8ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (4.66.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a6149f0-f6eb-42d5-af7b-52c0cb4f4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b88b156-437d-4b52-8e06-58cbf17869a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (4.66.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/u1e7bad41fdc17145d63dda8aedb93b1/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94394d14-a224-4bce-a539-670ac4edc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path('flickr8k')\n",
    "\n",
    "if len(list(path.rglob('*'))) < 16197:\n",
    "    tf.keras.utils.get_file(\n",
    "    origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
    "    cache_dir='.',\n",
    "    cache_subdir=path,\n",
    "    extract=True)\n",
    "    tf.keras.utils.get_file(\n",
    "    origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
    "    cache_dir='.',\n",
    "    cache_subdir=path,\n",
    "    extract=True)\n",
    "\n",
    "captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
    "captions = (line.split('\\t') for line in captions)\n",
    "captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
    "\n",
    "cap_dict = collections.defaultdict(list)\n",
    "for fname, cap in captions:\n",
    "    cap_dict[fname].append(cap)\n",
    "\n",
    "train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
    "train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
    "\n",
    "test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
    "test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
    "\n",
    "train_raw = tf.data.experimental.from_list(train_captions)\n",
    "test_raw = tf.data.experimental.from_list(test_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e568f4c-7ee3-4c3f-a5ce-3831683c6e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc9fa64-c032-4a2f-96b5-d8e00010d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE=(224, 224, 3)\n",
    "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=IMAGE_SHAPE,\n",
    "    include_top=False,\n",
    "    include_preprocessing=True)\n",
    "mobilenet.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c410200e-d377-44b9-820f-76e8ba2e49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dec4c3cc-754f-48f3-ab44-adb7d183aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(s):\n",
    "  s = tf.strings.lower(s)\n",
    "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
    "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8dd0d36-9946-44a8-b5cc-9b6cec4fc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the top 5000 words for a vocabulary.\n",
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    standardize=standardize,\n",
    "    ragged=True)\n",
    "# Learn the vocabulary from the caption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fce1e29-721c-47da-ac11-f94d681a6665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 03:33:53.610301: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type CPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))\n",
    "word_to_index = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)\n",
    "\n",
    "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
    "t\n",
    "\n",
    "w = index_to_word(t)\n",
    "w.to_list()\n",
    "\n",
    "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()\n",
    "\n",
    "def match_shapes(images, captions):\n",
    "  caption_shape = einops.parse_shape(captions, 'b c')\n",
    "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
    "  images = einops.repeat(\n",
    "      images, 'b ... -> (b c) ...',\n",
    "      c = caption_shape['c'])\n",
    "  return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e66d835-ef15-4f0d-90ef-dd9c92d64a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_txt(imgs, txts):\n",
    "  tokens = tokenizer(txts)\n",
    "\n",
    "  input_tokens = tokens[..., :-1]\n",
    "  label_tokens = tokens[..., 1:]\n",
    "  return (imgs, input_tokens), label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d96b598-9441-48dd-8b57-f9ede6fafada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .shuffle(10000)\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  def to_tensor(inputs, labels):\n",
    "    (images, in_tok), out_tok = inputs, labels\n",
    "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
    "\n",
    "  return (ds\n",
    "          .map(match_shapes, tf.data.AUTOTUNE)\n",
    "          .unbatch()\n",
    "          .shuffle(shuffle_buffer)\n",
    "          .batch(batch_size)\n",
    "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "          .map(to_tensor, tf.data.AUTOTUNE)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42465347-c388-42ff-a015-3aa1cf36754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2462862/1720203226.py:5: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 03:33:54.088047: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "188it [00:41,  4.51it/s]\n",
      "2024-02-08 03:34:36.933967: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "32it [00:07,  4.47it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  # Run the feature extractor on each batch\n",
    "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
    "  def gen():\n",
    "    for (images, captions) in tqdm.tqdm(ds): \n",
    "      feature_maps = image_model(images)\n",
    "\n",
    "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
    "      yield feature_maps, captions\n",
    "\n",
    "  # Wrap the generator in a new tf.data.Dataset.\n",
    "  new_ds = tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      output_signature=(\n",
    "          tf.TensorSpec(shape=image_model.output_shape),\n",
    "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
    "\n",
    "  # Apply the tokenization \n",
    "  new_ds = (new_ds\n",
    "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .shuffle(1000))\n",
    "\n",
    "  # Save the dataset into shard files.\n",
    "  def shard_func(i, item):\n",
    "    return i % shards\n",
    "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
    "\n",
    "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
    "  def custom_reader_func(datasets):\n",
    "    datasets = datasets.shuffle(1000)\n",
    "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
    "  \n",
    "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
    "\n",
    "  def drop_index(i, x):\n",
    "    return x\n",
    "\n",
    "  ds = (ds\n",
    "        .map(drop_index, tf.data.AUTOTUNE)\n",
    "        .shuffle(shuffle)\n",
    "        .padded_batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "  return ds\n",
    "\n",
    "save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n",
    "save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a843cf10-bcde-497f-89ea-99d070f008ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .shuffle(10000)\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  def to_tensor(inputs, labels):\n",
    "    (images, in_tok), out_tok = inputs, labels\n",
    "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
    "\n",
    "  return (ds\n",
    "          .map(match_shapes, tf.data.AUTOTUNE)\n",
    "          .unbatch()\n",
    "          .shuffle(shuffle_buffer)\n",
    "          .batch(batch_size)\n",
    "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "          .map(to_tensor, tf.data.AUTOTUNE)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8129ec17-f5ef-4b8d-bff6-8a045550f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare_dataset(train_raw, tokenizer)\n",
    "test_ds = prepare_dataset(test_raw, tokenizer)\n",
    "train_ds = load_dataset('train_cache')\n",
    "test_ds = load_dataset('test_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0e004f5-01d5-45ff-a37a-3a0ad169883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 03:38:35.264160: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "188it [01:29,  2.09it/s]\n",
      "2024-02-08 03:40:05.902358: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "32it [00:15,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  # Run the feature extractor on each batch\n",
    "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
    "  def gen():\n",
    "    for (images, captions) in tqdm.tqdm(ds): \n",
    "      feature_maps = image_model(images)\n",
    "\n",
    "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
    "      yield feature_maps, captions\n",
    "\n",
    "  # Wrap the generator in a new tf.data.Dataset.\n",
    "  new_ds = tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      output_signature=(\n",
    "          tf.TensorSpec(shape=image_model.output_shape),\n",
    "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
    "\n",
    "  # Apply the tokenization \n",
    "  new_ds = (new_ds\n",
    "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .shuffle(1000))\n",
    "\n",
    "  # Save the dataset into shard files.\n",
    "  def shard_func(i, item):\n",
    "    return i % shards\n",
    "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
    "\n",
    "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
    "  def custom_reader_func(datasets):\n",
    "    datasets = datasets.shuffle(1000)\n",
    "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
    "  \n",
    "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
    "\n",
    "  def drop_index(i, x):\n",
    "    return x\n",
    "\n",
    "  ds = (ds\n",
    "        .map(drop_index, tf.data.AUTOTUNE)\n",
    "        .shuffle(shuffle)\n",
    "        .padded_batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "  return ds\n",
    "\n",
    "save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n",
    "save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf2926d5-35b3-4ad7-a8c2-d196a16f4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset('train_cache')\n",
    "test_ds = load_dataset('test_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a73fe7b5-e3fd-40e5-b84b-734fd4a99787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=depth,\n",
    "        mask_zero=True)\n",
    "    \n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq,x])\n",
    "\n",
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    # Use Add instead of + so the keras mask propagates through.\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x,\n",
    "                    use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)\n",
    "\n",
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x, y, **kwargs):\n",
    "    attn, attention_scores = self.mha(\n",
    "             query=x, value=y,\n",
    "             return_attention_scores=True)\n",
    "    \n",
    "    self.last_attention_scores = attention_scores\n",
    "\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                          key_dim=units,\n",
    "                                          dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "      \n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    # Text input\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "    \n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq\n",
    "\n",
    "#@title\n",
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(\n",
    "        units=tokenizer.vocabulary_size(), **kwargs)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "    self.bias = None\n",
    "\n",
    "  def adapt(self, ds):\n",
    "    counts = collections.Counter()\n",
    "    vocab_dict = {name: id \n",
    "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "\n",
    "    for tokens in tqdm.tqdm(ds):\n",
    "      counts.update(tokens.numpy().flatten())\n",
    "\n",
    "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "\n",
    "    counts_arr = counts_arr[:]\n",
    "    for token in self.banned_tokens:\n",
    "      counts_arr[vocab_dict[token]] = 0\n",
    "\n",
    "    total = counts_arr.sum()\n",
    "    p = counts_arr/total\n",
    "    p[counts_arr==0] = 1.0\n",
    "    log_p = np.log(p)  # log(1) == 0\n",
    "\n",
    "    entropy = -(log_p*p).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "\n",
    "    self.bias = log_p\n",
    "    self.bias[counts_arr==0] = -1e9\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    # TODO(b/250038731): Fix this.\n",
    "    # An Add layer doesn't work because of the different shapes.\n",
    "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
    "    # the losses.\n",
    "    return x + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "628d7169-d6ce-495a-a50f-e01195d88ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:04<00:00, 233.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniform entropy: 8.52\n",
      "Marginal entropy: 5.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
    "# This might run a little faster if the dataset didn't also have to load the image data.\n",
    "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "482657a6-4226-4884-b32b-7e04239e7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word_to_index = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary())\n",
    "    self.index_to_word = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary(),\n",
    "        invert=True) \n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        depth=units,\n",
    "        max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8dd0a26-9617-4964-b42d-6fcd8f35b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "  @Captioner.add_method\n",
    "  def call(self, inputs):\n",
    "    image, txt = inputs\n",
    "\n",
    "    if image.shape[-1] == 3:\n",
    "      # Apply the feature-extractor, if you get an RGB image.\n",
    "      image = self.feature_extractor(image)\n",
    "    \n",
    "    # Flatten the feature map\n",
    "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
    "\n",
    "\n",
    "    if txt.dtype == tf.string:\n",
    "      # Apply the tokenizer if you get string inputs.\n",
    "      txt = tokenizer(txt)\n",
    "\n",
    "    txt = self.seq_embedding(txt)\n",
    "\n",
    "    # Look at the image\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      txt = dec_layer(inputs=(image, txt))\n",
    "      \n",
    "    txt = self.output_layer(txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72223edb-30f1-4b4e-967a-9de1918efa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
    "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03e025ff-2d08-4c27-ab65-3f4413aae243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://tensorflow.org/images/surf.jpg\n",
      "64400/64400 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
    "image = load_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c83f315b-54c0-457f-a367-5e794471f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "a on a dog a a\n",
      "a beside light on elephant side a his camouflage crossing a dogs court red are of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@Captioner.add_method\n",
    "def simple_gen(self, image, temperature=1):\n",
    "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
    "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
    "\n",
    "  tokens = initial # (batch, sequence)\n",
    "  for n in range(50):\n",
    "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
    "    preds = preds[:,-1, :]  #(batch, vocab)\n",
    "    if temperature==0:\n",
    "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
    "    else:\n",
    "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
    "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
    "\n",
    "    if next[0] == self.word_to_index('[END]'):\n",
    "      break\n",
    "  words = index_to_word(tokens[0, 1:-1])\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  return result.numpy().decode()\n",
    "\n",
    "def masked_loss(labels, preds):  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
    "\n",
    "  mask = (labels != 0) & (loss < 1e8) \n",
    "  mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "  loss = loss*mask\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "def masked_acc(labels, preds):\n",
    "  mask = tf.cast(labels!=0, tf.float32)\n",
    "  preds = tf.argmax(preds, axis=-1)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  match = tf.cast(preds == labels, mask.dtype)\n",
    "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
    "  return acc\n",
    "\n",
    "class GenerateText(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
    "    self.image = load_image(image_path)\n",
    "\n",
    "  def on_epoch_end(self, epochs=None, logs=None):\n",
    "    print()\n",
    "    print()\n",
    "    for t in (0.0, 0.5, 1.0):\n",
    "      result = self.model.simple_gen(self.image, temperature=t)\n",
    "      print(result)\n",
    "    print()\n",
    "\n",
    "g = GenerateText()\n",
    "g.model = model\n",
    "g.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "187f6dc0-6757-4882-bdf0-f73548d8bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    GenerateText(),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, restore_best_weights=True)]\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "           loss=masked_loss,\n",
    "           metrics=[masked_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d83f7-16c7-4d8d-85f3-e06122e80be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.9950 - masked_acc: 0.2026\n",
      "\n",
      "a man in a man in a man\n",
      "a man in a man and a a\n",
      "an in forest a shaved\n",
      "\n",
      "100/100 [==============================] - 119s 1s/step - loss: 4.9950 - masked_acc: 0.2026 - val_loss: 4.6312 - val_masked_acc: 0.2420\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.6535 - masked_acc: 0.2529\n",
      "\n",
      "a man in a man in the water\n",
      "a man in a man in a water\n",
      "two subway along the through the grass\n",
      "\n",
      "100/100 [==============================] - 80s 797ms/step - loss: 4.6535 - masked_acc: 0.2529 - val_loss: 4.3381 - val_masked_acc: 0.2748\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.3921 - masked_acc: 0.2771\n",
      "\n",
      "a man in a red shirt is in the water\n",
      "a man is in a in a black and a black\n",
      "red skateboarder girl in the field leaps ride\n",
      "\n",
      "100/100 [==============================] - 48s 470ms/step - loss: 4.3921 - masked_acc: 0.2771 - val_loss: 4.1336 - val_masked_acc: 0.2907\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.2555 - masked_acc: 0.2963\n",
      "\n",
      "a man in a red shirt is in the water\n",
      "a man in a red shirt and a water\n",
      "a young boy to a leafy red looking lying in goal\n",
      "\n",
      "100/100 [==============================] - 111s 1s/step - loss: 4.2555 - masked_acc: 0.2963 - val_loss: 3.9628 - val_masked_acc: 0.3073\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.0995 - masked_acc: 0.3058\n",
      "\n",
      "a man in a white shirt is running in the water\n",
      "a man is sitting on a water\n",
      "the man climb a pool\n",
      "\n",
      "100/100 [==============================] - 150s 1s/step - loss: 4.0995 - masked_acc: 0.3058 - val_loss: 3.9341 - val_masked_acc: 0.3185\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.0307 - masked_acc: 0.3141\n",
      "\n",
      "a man in a blue shirt is running through the water\n",
      "a man in the water\n",
      "a blanket in the bubble the skier in the beach behind sand\n",
      "\n",
      "100/100 [==============================] - 69s 681ms/step - loss: 4.0307 - masked_acc: 0.3141 - val_loss: 3.8888 - val_masked_acc: 0.3201\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.9208 - masked_acc: 0.3295\n",
      "\n",
      "a man in a blue shirt is jumping in the water\n",
      "a man is jumping in the water\n",
      "a man in the water\n",
      "\n",
      "100/100 [==============================] - 72s 719ms/step - loss: 3.9208 - masked_acc: 0.3295 - val_loss: 3.7488 - val_masked_acc: 0.3344\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.8812 - masked_acc: 0.3301\n",
      "\n",
      "a man in a blue shirt is jumping in the water\n",
      "a man is wearing a blue shirt is riding the water\n",
      "a black man in the side of shoes man sits at a\n",
      "\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 3.8812 - masked_acc: 0.3301 - val_loss: 3.7713 - val_masked_acc: 0.3307\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.7817 - masked_acc: 0.3356\n",
      "\n",
      "a man in a blue shirt is jumping over a pool\n",
      "a person in a red path\n",
      "a little boy wearing a swimming down four cheek a ball\n",
      "\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 3.7817 - masked_acc: 0.3356 - val_loss: 3.6331 - val_masked_acc: 0.3400\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.7281 - masked_acc: 0.3381\n",
      "\n",
      "a man in a blue shirt is jumping over a pool\n",
      "a young boy is jumping in the water\n",
      "woman rides a pool in the beach\n",
      "\n",
      "100/100 [==============================] - 83s 818ms/step - loss: 3.7281 - masked_acc: 0.3381 - val_loss: 3.5878 - val_masked_acc: 0.3463\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.6151 - masked_acc: 0.3486\n",
      "\n",
      "a man in a blue shirt is jumping over a pool\n",
      "a boy in a red shirt is doing a surfboard\n",
      "a young man under a uniform on snowy wave\n",
      "\n",
      "100/100 [==============================] - 53s 523ms/step - loss: 3.6151 - masked_acc: 0.3486 - val_loss: 3.5152 - val_masked_acc: 0.3523\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.6145 - masked_acc: 0.3458\n",
      "\n",
      "a man in a blue shirt is jumping into the water\n",
      "a man in a blue shirt is playing in a swimming\n",
      "a man is swimming in some water\n",
      "\n",
      "100/100 [==============================] - 50s 489ms/step - loss: 3.6145 - masked_acc: 0.3458 - val_loss: 3.4258 - val_masked_acc: 0.3582\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5883 - masked_acc: 0.3505\n",
      "\n",
      "a man in a blue shirt is in the water\n",
      "a man in a blue is walking through the water\n",
      "the surfer is watch from ocean\n",
      "\n",
      "100/100 [==============================] - 47s 462ms/step - loss: 3.5883 - masked_acc: 0.3505 - val_loss: 3.5151 - val_masked_acc: 0.3479\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5169 - masked_acc: 0.3572\n",
      "\n",
      "a man in a red shirt is swimming pool\n",
      "a man in the water on the water\n",
      "a person on jagged shirt riding up the sandy up in a pedestrian from a beach\n",
      "\n",
      "100/100 [==============================] - 53s 517ms/step - loss: 3.5169 - masked_acc: 0.3572 - val_loss: 3.4718 - val_masked_acc: 0.3537\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5000 - masked_acc: 0.3615\n",
      "\n",
      "a man in a blue shirt is jumping into the water\n",
      "a young girl in a ocean\n",
      "the skateboarder walks in blue mouth while mouths is as he through some shore\n",
      "\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 3.5000 - masked_acc: 0.3615 - val_loss: 3.4476 - val_masked_acc: 0.3530\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.4367 - masked_acc: 0.3633\n",
      "\n",
      "a man in a blue shirt is swimming pool\n",
      "a man in a white shirt is riding a pool\n",
      "a man and a large sled sets in the water\n",
      "\n",
      "100/100 [==============================] - 51s 495ms/step - loss: 3.4367 - masked_acc: 0.3633 - val_loss: 3.3259 - val_masked_acc: 0.3610\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.4355 - masked_acc: 0.3608\n",
      "\n",
      "a man in a blue shirt is swimming pool\n",
      "a man in a blue coat is in the wave\n",
      "a man shirt jumping up a in the water at the above the water\n",
      "\n",
      "100/100 [==============================] - 64s 637ms/step - loss: 3.4355 - masked_acc: 0.3608 - val_loss: 3.3924 - val_masked_acc: 0.3551\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.3914 - masked_acc: 0.3669\n",
      "\n",
      "a man in a red shirt is swimming pool\n",
      "a man is swimming in the ocean\n",
      "two boys are splashing trunks being structure\n",
      "\n",
      "100/100 [==============================] - 53s 519ms/step - loss: 3.3914 - masked_acc: 0.3669 - val_loss: 3.3436 - val_masked_acc: 0.3646\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.3858 - masked_acc: 0.3667\n",
      "\n",
      "a man in a blue shirt is riding a wave\n",
      "a man in a blue shirt is wearing a wave\n",
      "a snowboarder is holding a pool\n",
      "\n",
      "100/100 [==============================] - 61s 606ms/step - loss: 3.3858 - masked_acc: 0.3667 - val_loss: 3.3604 - val_masked_acc: 0.3635\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.2898 - masked_acc: 0.3746\n",
      "\n",
      "a man in a blue shirt is swimming pool\n",
      "a person in a blue wave in a blue water\n",
      "a man in a blue wave on a ski crouches is pitching\n",
      "\n",
      "100/100 [==============================] - 61s 609ms/step - loss: 3.2898 - masked_acc: 0.3746 - val_loss: 3.2205 - val_masked_acc: 0.3742\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.2764 - masked_acc: 0.3755\n",
      "\n",
      "a man in a blue shirt is swimming pool\n",
      "a boy is swimming in the water in the water\n",
      "a white and white plays in a kayak and swimming beach and two field\n",
      "\n",
      "100/100 [==============================] - 60s 594ms/step - loss: 3.2764 - masked_acc: 0.3755 - val_loss: 3.2458 - val_masked_acc: 0.3703\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.2966 - masked_acc: 0.3680\n",
      "\n",
      "a man in a red shirt is swimming in the water\n",
      "a man riding a surfboard in the water\n",
      "a wetsuit wearing a red shirt riding a creek\n",
      "\n",
      "100/100 [==============================] - 58s 571ms/step - loss: 3.2966 - masked_acc: 0.3680 - val_loss: 3.1948 - val_masked_acc: 0.3688\n",
      "Epoch 23/30\n",
      " 16/100 [===>..........................] - ETA: 2:14 - loss: 3.1875 - masked_acc: 0.3813"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(),\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=test_ds.repeat(),\n",
    "    validation_steps=20,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d421c2-8cb1-4d53-84ca-43843de00cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
